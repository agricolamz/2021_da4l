[["коэффициент-байеса.html", "7 Коэффициент Байеса 7.1 Формула Байеса опять 7.2 Категориальные данные 7.3 Интерпретация коэфициента Байеса 7.4 Биномиальные данные", " 7 Коэффициент Байеса 7.1 Формула Байеса опять \\[P(\\theta|Data) = \\frac{P(Data|\\theta) \\times P(\\theta) }{P(Data)}\\] Рассмотрим какой-то простой случай, который мы уже видели много раз. Немного упрощая данные из статьи (Rosenbach 2003: 394), можно сказать что носители британского английского предпочитают s-генитив (90%) of-генитиву (10%). Проведите байесовский апдейт, если Вы наблюдаете в интервью британского актера из 120 контекстов 92 s-генитивов. Априорное распределение берите соразмерное данным. Если мы не будем следовать простой дорожкой, которую мы обсуждали несколько разделов назад, а будем все делать согласно формуле Байеса, то получатся следующие компоненты: априорное распределение tibble(x = seq(0, 1, 0.001), prior = dbeta(x = x, shape1 = 120*0.9, shape2 = 120*0.1)) %&gt;% ggplot(aes(x, prior))+ geom_line(color = &quot;red&quot;) функция правдоподобия tibble(x = seq(0, 1, 0.001), likelihood = dbinom(x = 92, size = 120, prob = x)) %&gt;% ggplot(aes(x, likelihood))+ geom_line() их произведение (пропорционально апостериорному распределению) tibble(x = seq(0, 1, 0.001), prior = dbeta(x = x, shape1 = 120*0.9, shape2 = 120*0.1), likelihood = dbinom(x = 92, size = 120, prob = x), product = prior*likelihood) %&gt;% ggplot(aes(x, product))+ geom_line() предельное правдоподобие, которое позволяет сделать получившееся распределение распределением вероятностей marginal_likelihood &lt;- integrate(function(p){ dbinom(92, 120, p) * dbeta(p, 120*0.9, 120*0.1)}, lower = 0, upper = 1) marginal_likelihood 0.0009531395 with absolute error &lt; 0.000044 … и в результате получается апостериорное распределение! tibble(x = seq(0, 1, 0.001), prior = dbeta(x = x, shape1 = 120*0.9, shape2 = 120*0.1), likelihood = dbinom(x = 92, size = 120, prob = x), product = prior*likelihood, posterior = product/marginal_likelihood[[1]]) %&gt;% ggplot(aes(x, posterior))+ geom_line(color = &quot;darkgreen&quot;)+ geom_line(aes(y = prior), color = &quot;red&quot;) … которое мы умеем доставать и быстрее: tibble(x = seq(0, 1, 0.001), prior = dbeta(x = x, shape1 = 120*0.9, shape2 = 120*0.1), likelihood = dbinom(x = 92, size = 120, prob = x), product = prior*likelihood, posterior = product/marginal_likelihood[[1]], posterior_2 = dbeta(x = x, shape1 = 120*0.9+92, shape2 = 120*0.1+120-92)) %&gt;% ggplot(aes(x, posterior))+ geom_line(color = &quot;darkgreen&quot;, size = 2)+ geom_line(aes(y = prior), color = &quot;red&quot;)+ geom_line(aes(y = posterior_2), linetype = 2, color = &quot;yellow&quot;) Представим себе, что у нас есть \\(k\\) гипотез \\(M\\). Тогда формула Байеса может выглядеть вот так: \\[P(M_k|Data) = \\frac{P(Data|M_k) \\times P(M_k) }{P(Data)}\\] В данном занятии мы рассмотрим только слычай двух модели, но можно рассматривать и случаи, когда моделей много. Посмотрим на соотношение апосториорных распределений двух моделей: \\[\\underbrace{\\frac{P(M_1 \\mid Data)}{P(M_2 \\mid Data)}}_{\\text{posterior odds}} = \\frac{\\frac{P(Data|M_1) \\times P(M_1) }{P(Data)}}{\\frac{P(Data|M_2) \\times P(M_2) }{P(Data)}}=\\underbrace{\\frac{P(Data \\mid M_1)}{P(Data \\mid M_2)}}_{\\text{Bayes factor}}\\times\\underbrace{\\frac{P(M_1)}{P(M_2)}}_{\\text{prior odds}}\\] Таким образом байесовский коэффициент это соотношение апосториорных распределений деленное на соотношение априорных распределений. \\[BF_{12}= \\frac{P(M_1 \\mid Data)/P(M_2 \\mid Data)}{P(M_1)/P(M_2)}=\\frac{P(M_1 \\mid Data)\\times P(M_2)}{P(M_2 \\mid Data)\\times P(M_1)}\\] В результате получается, что коэффициент Байеса – это соотношение предельных правдоподобий (знаменатель теоремы Байеса): \\[BF_{12}= \\frac{P(Data|\\theta, M_1))}{P(Data|\\theta, M_2))}=\\frac{\\int P(Data|\\theta, M_1)\\times P(\\theta|M_1)}{\\int P(Data|\\theta, M_2)\\times P(\\theta|M_2)}\\] Важно заметить, что если вероятности априорных моделей равны, то байесовский коэффициент равен просто соотношению функций правдоподобия. Надо отметить, что не все тепло относятся к сравнению моделей байесовским коэффициентом (см. Gelman, Rubin 1994). 7.2 Категориальные данные Для примера обратися снова к датасету, который содержит спамерские и обычные смс-сообщения, выложенный UCI Machine Learning на kaggle и при помощи пакета udpipe токенизировал и определил часть речи: sms_pos &lt;- read_csv(&quot;https://raw.githubusercontent.com/agricolamz/2021_da4l/master/data/spam_sms_pos.csv&quot;) glimpse(sms_pos) Rows: 34 Columns: 3 $ type &lt;chr&gt; &quot;ham&quot;, &quot;ham&quot;, &quot;ham&quot;, &quot;ham&quot;, &quot;ham&quot;, &quot;ham&quot;, &quot;ham&quot;, &quot;ham&quot;, &quot;ham&quot;, &quot;… $ upos &lt;chr&gt; &quot;ADJ&quot;, &quot;ADP&quot;, &quot;ADV&quot;, &quot;AUX&quot;, &quot;CCONJ&quot;, &quot;DET&quot;, &quot;INTJ&quot;, &quot;NOUN&quot;, &quot;NUM… $ n &lt;dbl&gt; 4329, 5004, 5832, 5707, 1607, 3493, 1676, 12842, 1293, 2424, 114… sms_pos %&gt;% group_by(type) %&gt;% mutate(ratio = n/sum(n), upos = fct_reorder(upos, n, mean, .desc = TRUE)) %&gt;% ggplot(aes(type, ratio))+ geom_col()+ geom_label(aes(label = round(ratio, 3)), position = position_stack(vjust = 0.5))+ facet_wrap(~upos, scales = &quot;free_y&quot;) Давайте полученные доли считать нашей моделью: сумма всех чисел внутри каждого типа (ham/spam) дает в сумме 1. Мы получили новое сообщение: Call FREEPHONE 0800 542 0825 now! Модель udpipe разобрала его следующим образом: VERB NUM NUM NUM NUM ADV PUNCT Если мы считаем наши модели равновероятными: first_update &lt;- tibble(model = c(&quot;ham&quot;, &quot;spam&quot;), prior = 0.5, likelihood = c(0.135, 0.096), product = prior*likelihood, marginal_likelihood = sum(product), posterior = product/marginal_likelihood) first_update Если же мы примем во внимание, что наши классы не равноправны, то сможем посчитать это нашим априорным распределением для моделей. sms_pos %&gt;% uncount() %&gt;% count(type) %&gt;% mutate(ratio = n/sum(n)) -&gt; class_ratio class_ratio second_update &lt;- tibble(model = c(&quot;ham&quot;, &quot;spam&quot;), prior = class_ratio$ratio, likelihood = c(0.135, 0.096), product = prior*likelihood, marginal_likelihood = sum(product), posterior = product/marginal_likelihood) second_update # Bayes factor second_update$marginal_likelihood[1]/first_update$marginal_likelihood[1] [1] 1.098469 7.3 Интерпретация коэфициента Байеса 7.4 Биномиальные данные Рассмотрим простенькую задачу, которую мы видели раньше: Немного упрощая данные из статьи (Rosenbach 2003: 394), можно сказать что носители британского английского предпочитают s-генитив (90%) of-генитиву (10%), а носители американского английского предпочитают s-генитив (85%) of-генитиву (15%). Мы наблюдаем актера, который в интервью из 120 контекстов использует в 92 случаях s-генитивы. Сравните модели при помощи байесовского коэффециента. tibble(x = seq(0, 1, by = 0.001), y = dbeta(x, 120*0.9, 120*0.1), z = dbeta(x, 120*0.85, 120*0.15)) %&gt;% ggplot(aes(x, y))+ geom_line(color = &quot;red&quot;)+ geom_line(aes(y = z), color = &quot;lightblue&quot;)+ geom_vline(xintercept = 92/120, linetype = 2) m1 &lt;- function(p) dbinom(92, 120, p) * dbeta(p, 120*0.9, 120*0.1) m2 &lt;- function(p) dbinom(92, 120, p) * dbeta(p, 120*0.85, 120*0.15) integrate(m1, 0, 1)[[1]]/integrate(m2, 0, 1)[[1]] [1] 0.0672068 В работе (Coretta 2016) собраны данные длительности исландских гласных. Отфильтруйте данные, произнесенные носителем tt01 (переменная speaker), посчитайте байесовский коэффициент (\\(B_{12}\\)) для двух априорных моделей: нормального распределения со средним 87 и стандартным отклонением 25. (\\(m_1\\)) нормального распределения со средним 85 и стандартным отклонением 30. (\\(m_2\\)) Ответ округлите до трёх или менее знаков после запятой. "]]

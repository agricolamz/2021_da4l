[["логистическая-и-мультиномиальная-регрессия.html", "10 Логистическая и мультиномиальная регрессия 10.1 Логистическая регрессия", " 10 Логистическая и мультиномиальная регрессия library(tidyverse) Логистическая (logit, logistic) и мультиномиальная (multinomial) регрессия применяются в случаях, когда зависимая переменная является категориальной: с двумя значениями (логистическая регрессия) с более чем двумя значениями (мультиномиальная регрессия) 10.1 Логистическая регрессия 10.1.1 Теория Мы хотим чего-то такого: \\[\\underbrace{y}_{[-\\infty, +\\infty]}=\\underbrace{\\mbox{β}_0+\\mbox{β}_1\\cdot x_1+\\mbox{β}_2\\cdot x_2 + \\dots +\\mbox{β}_k\\cdot x_k +\\mbox{ε}_i}_{[-\\infty, +\\infty]}\\] Вероятность — отношение количества успехов к общему числу событий: \\[p = \\frac{\\mbox{# успехов}}{\\mbox{# неудач} + \\mbox{# успехов}}, p \\in [0, 1]\\] Шансы — отношение количества успехов к количеству неудач: \\[odds = \\frac{p}{1-p} = \\frac{p\\mbox{(успеха)}}{p\\mbox{(неудачи)}}, odds \\in [0, +\\infty]\\] Натуральный логарифм шансов: \\[\\log(odds) \\in [-\\infty, +\\infty]\\] Но, что нам говорит логарифм шансов? Как нам его интерпретировать? tibble(n = 10, success = 1:9, failure = n - success, prob.1 = success/(success+failure), odds = success/failure, log_odds = log(odds), prob.2 = exp(log_odds)/(1+exp(log_odds))) Как связаны вероятность и логарифм шансов: \\[\\log(odds) = \\log\\left(\\frac{p}{1-p}\\right)\\] \\[p = \\frac{\\exp(\\log(odds))}{1+\\exp(\\log(odds))}\\] Логарифм шансов равен 0.25. Посчитайте вероятность успеха: Как связаны вероятность и логарифм шансов: 10.1.2 Практика В датасет собрано 19 языков, со следующими переменными: language — переменная, содержащая язык tone — бинарная переменная, обозначающая наличие тонов long_vowels — бинарная переменная, обозначающая наличие долгих гласных stress — бинарная переменная, обозначающая наличие ударения ejectives — бинарная переменная, обозначающая наличие абруптивных consonants — переменная, содержащая информацию о количестве согласных vowels — переменная, содержащая информацию о количестве гласных phonological_profiles &lt;- read_csv(&quot;https://raw.githubusercontent.com/agricolamz/2021_da4l/master/data/phonological_profiles.csv&quot;) glimpse(phonological_profiles) ## Rows: 19 ## Columns: 7 ## $ language &lt;chr&gt; &quot;Turkish&quot;, &quot;Korean&quot;, &quot;Tiwi&quot;, &quot;Liberia Kpelle&quot;, &quot;Tulu&quot;, &quot;M… ## $ tone &lt;lgl&gt; FALSE, FALSE, FALSE, TRUE, FALSE, FALSE, TRUE, FALSE, TRU… ## $ long_vowels &lt;lgl&gt; TRUE, FALSE, FALSE, FALSE, TRUE, FALSE, TRUE, FALSE, TRUE… ## $ stress &lt;lgl&gt; TRUE, TRUE, TRUE, FALSE, FALSE, TRUE, FALSE, TRUE, TRUE, … ## $ ejectives &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, F… ## $ consonants &lt;dbl&gt; 25, 21, 22, 22, 24, 20, 22, 24, 15, 18, 17, 8, 26, 28, 30… ## $ vowels &lt;dbl&gt; 8, 11, 4, 12, 13, 6, 20, 12, 5, 11, 8, 5, 14, 6, 7, 7, 5,… set.seed(42) phonological_profiles %&gt;% ggplot(aes(ejectives, consonants))+ geom_boxplot(aes(fill = ejectives), show.legend = FALSE, outlier.alpha = 0)+ # по умолчанию боксплот рисует выбросы, outlier.alpha = 0 -- это отключает geom_jitter(size = 3) 10.1.2.1 Почему не линейную регрессию? lm_0 &lt;- lm(as.double(ejectives)~1, data = phonological_profiles) lm_1 &lt;- lm(as.double(ejectives)~consonants, data = phonological_profiles) lm_0 ## ## Call: ## lm(formula = as.double(ejectives) ~ 1, data = phonological_profiles) ## ## Coefficients: ## (Intercept) ## 0.3158 lm_1 ## ## Call: ## lm(formula = as.double(ejectives) ~ consonants, data = phonological_profiles) ## ## Coefficients: ## (Intercept) consonants ## -0.5389 0.0353 Первая модель: \\[ejectives = 0.3158 \\times consonants\\] Вторая модель: \\[ejectives = -0.5389 + 0.0353 \\times consonants\\] phonological_profiles %&gt;% ggplot(aes(consonants, as.double(ejectives)))+ geom_point()+ geom_smooth(method = &quot;lm&quot;)+ theme_bw()+ labs(y = &quot;ejectives (yes = 2, no = 1)&quot;) 10.1.2.2 Логит: модель без предиктора logit_0 &lt;- glm(ejectives~1, family = &quot;binomial&quot;, data = phonological_profiles) summary(logit_0) ## ## Call: ## glm(formula = ejectives ~ 1, family = &quot;binomial&quot;, data = phonological_profiles) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -0.8712 -0.8712 -0.8712 1.5183 1.5183 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -0.7732 0.4935 -1.567 0.117 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 23.699 on 18 degrees of freedom ## Residual deviance: 23.699 on 18 degrees of freedom ## AIC: 25.699 ## ## Number of Fisher Scoring iterations: 4 logit_0$coefficients ## (Intercept) ## -0.7731899 table(phonological_profiles$ejectives) ## ## FALSE TRUE ## 13 6 log(6/13) # β0 ## [1] -0.7731899 6/(13+6) # p ## [1] 0.3157895 exp(log(6/13))/(1+exp(log(6/13))) # p ## [1] 0.3157895 Какой коэфициент логистической регрессии, мы получим, запустив модель, предсказывающую количество s-генитивов, если наши данные состоят из 620 s-генитивов из 699 генетивных контекстов? Ответ округлите до трех и меньше знаков после запятой. 10.1.2.3 Логит: модель c одним числовым предиктором logit_1 &lt;- glm(ejectives~consonants, family = &quot;binomial&quot;, data = phonological_profiles) summary(logit_1) ## ## Call: ## glm(formula = ejectives ~ consonants, family = &quot;binomial&quot;, data = phonological_profiles) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -1.08779 -0.49331 -0.20265 0.02254 2.45384 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -12.1123 6.1266 -1.977 0.0480 * ## consonants 0.4576 0.2436 1.878 0.0603 . ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 23.699 on 18 degrees of freedom ## Residual deviance: 12.192 on 17 degrees of freedom ## AIC: 16.192 ## ## Number of Fisher Scoring iterations: 6 logit_1$coefficients ## (Intercept) consonants ## -12.1123347 0.4576095 phonological_profiles %&gt;% mutate(ejectives = as.double(ejectives)) %&gt;% ggplot(aes(consonants, ejectives)) + geom_smooth(method = &quot;glm&quot;, method.args = list(family = &quot;binomial&quot;), se = FALSE)+ geom_point() Какова вероятность, что в языке с 29 согласными есть абруптивные? logit_1$coefficients ## (Intercept) consonants ## -12.1123347 0.4576095 \\[\\log\\left({\\frac{p}{1-p}}\\right)_i=\\beta_0+\\beta_1\\times consinants_i + \\epsilon_i\\] \\[\\log\\left({\\frac{p}{1-p}}\\right)=-12.1123347 + 0.4576095 \\times 29 = 1.158341\\] \\[p = \\frac{e^{1.158341}}{1+e^{1.158341}} = 0.7610311\\] # log(odds) predict(logit_1, newdata = data.frame(consonants = 29)) ## 1 ## 1.158341 # p predict(logit_1, newdata = data.frame(consonants = 29), type = &quot;response&quot;) ## 1 ## 0.7610312 Какой логорифм шансов предсказывает наша модель для языка с 25 согласными (6 знаков после запятой)? Какую вероятность предсказывает наша модель для языка с 25 согласными (6 знаков после запятой)? "]]

[["коэффициент-байеса.html", "7 Коэффициент Байеса 7.1 Формула Байеса опять 7.2 Категориальные данные 7.3 Интерпретация коэфициента Байеса 7.4 Биномиальные данные", " 7 Коэффициент Байеса 7.1 Формула Байеса опять Представим себе, что у нас есть \\(k\\) гипотез \\(M\\). Тогда формула Байеса может выглядеть вот так: \\[P(M_k|Data) = \\frac{P(Data|M_k) \\times P(M_k) }{P(Data)}\\] В данном занятии мы рассмотрим только слычай двух модели, но можно рассматривать и случаи, когда моделей много. Посмотрим на соотношение апосториорных распределений двух моделей: \\[\\underbrace{\\frac{P(M_1 \\mid Data)}{P(M_2 \\mid Data)}}_{\\text{posterior odds}} = \\frac{\\frac{P(Data|M_1) \\times P(M_1) }{P(Data)}}{\\frac{P(Data|M_2) \\times P(M_2) }{P(Data)}}=\\underbrace{\\frac{P(Data \\mid M_1)}{P(Data \\mid M_2)}}_{\\text{Bayes factor}}\\times\\underbrace{\\frac{P(M_1)}{P(M_2)}}_{\\text{prior odds}}\\] Таким образом байесовский коэффициент это соотношение апосториорных распределений деленное на соотношение априорных распределений. \\[BF_{12}= \\frac{P(M_1 \\mid Data)/P(M_2 \\mid Data)}{P(M_1)/P(M_2)}=\\frac{P(M_1 \\mid Data)\\times P(M_2)}{P(M_2 \\mid Data)\\times P(M_1)}\\] В результате получается, что коэффициент Байеса – это соотношение предельных правдоподобий (числитель теоремы Байеса): \\[BF_{12}= \\frac{P(Data|\\theta, M_1))}{P(Data|\\theta, M_2))}=\\frac{\\int P(Data|\\theta, M_1)\\times P(\\theta|M_1)}{\\int P(Data|\\theta, M_2)\\times P(\\theta|M_2)}\\] Важно заметить, что если вероятности априорных моделей равны, то байесовский коэффициент равен просто соотношению функций правдоподобия. Надо отметить, что не все тепло относятся к сравнению моделей байесовским коэффициентом (см. Gelman, Rubin 1994). 7.2 Категориальные данные Для примера обратися снова к датасету, который содержит спамерские и обычные смс-сообщения, выложенный UCI Machine Learning на kaggle и при помощи пакета udpipe токенизировал и определил часть речи: sms_pos &lt;- read_csv(&quot;https://raw.githubusercontent.com/agricolamz/2021_da4l/master/data/spam_sms_pos.csv&quot;) glimpse(sms_pos) Rows: 34 Columns: 3 $ type &lt;chr&gt; &quot;ham&quot;, &quot;ham&quot;, &quot;ham&quot;, &quot;ham&quot;, &quot;ham&quot;, &quot;ham&quot;, &quot;ham&quot;, &quot;ham&quot;, &quot;ham&quot;, &quot;… $ upos &lt;chr&gt; &quot;ADJ&quot;, &quot;ADP&quot;, &quot;ADV&quot;, &quot;AUX&quot;, &quot;CCONJ&quot;, &quot;DET&quot;, &quot;INTJ&quot;, &quot;NOUN&quot;, &quot;NUM… $ n &lt;dbl&gt; 4329, 5004, 5832, 5707, 1607, 3493, 1676, 12842, 1293, 2424, 114… sms_pos %&gt;% group_by(type) %&gt;% mutate(ratio = n/sum(n), upos = fct_reorder(upos, n, mean, .desc = TRUE)) %&gt;% ggplot(aes(type, ratio))+ geom_col()+ geom_label(aes(label = round(ratio, 3)), position = position_stack(vjust = 0.5))+ facet_wrap(~upos, scales = &quot;free_y&quot;) Давайте полученные доли считать нашей моделью: сумма всех чисел внутри каждого типа (ham/spam) дает в сумме 1. Мы получили новое сообщение: Call FREEPHONE 0800 542 0825 now! Модель udpipe разобрала его следующим образом: VERB NUM NUM NUM NUM ADV PUNCT Если мы считаем наши модели равновероятными: first_update &lt;- tibble(model = c(&quot;ham&quot;, &quot;spam&quot;), prior = 0.5, likelihood = c(0.135, 0.096), product = prior*likelihood, posterior = product/sum(product)) first_update # Bayes factor first_update$product[1]/first_update$product[2] [1] 1.40625 Отметим, что так как мы принимаем, что наши модели равновероятны, то байесовский коэфициент не отличается от частоного фукнций правдоподобия: first_update$likelihood[1]/first_update$likelihood[2] [1] 1.40625 Если же мы примем во внимание, что наши классы не равноправны, то сможем посчитать это нашим априорным распределением для моделей. sms_pos %&gt;% uncount() %&gt;% count(type) %&gt;% mutate(ratio = n/sum(n)) -&gt; class_ratio class_ratio second_update &lt;- tibble(model = c(&quot;ham&quot;, &quot;spam&quot;), prior = class_ratio$ratio, likelihood = c(0.135, 0.096), product = prior*likelihood, posterior = product/sum(product)) second_update # Bayes factor second_update$product[1]/second_update$product[2] [1] 5.34222 7.3 Интерпретация коэфициента Байеса 7.4 Биномиальные данные Рассмотрим простенькую задачу, которую мы видели раньше: Немного упрощая данные из статьи (Rosenbach 2003: 394), можно сказать что носители британского английского предпочитают s-генитив (90%) of-генитиву (10%), а носители американского английского предпочитают s-генитив (85%) of-генитиву (15%). Мы наблюдаем актера, который в интервью из 120 контекстов использует в 92 случаях s-генитивы. Сравните модели при помощи байесовского коэффециента. tibble(x = seq(0, 1, by = 0.001), y = dbeta(x, 120*0.9, 120*0.1), z = dbeta(x, 120*0.85, 120*0.15)) %&gt;% ggplot(aes(x, y))+ geom_line(color = &quot;red&quot;)+ geom_line(aes(y = z), color = &quot;lightblue&quot;)+ geom_vline(xintercept = 92/120, linetype = 2) m1 &lt;- function(p) dbinom(92, 120, p) * dbeta(p, 120*0.9, 120*0.1) m2 &lt;- function(p) dbinom(92, 120, p) * dbeta(p, 120*0.85, 120*0.15) integrate(m1, 0, 1)[[1]]/integrate(m2, 0, 1)[[1]] [1] 0.0672068 В работе (Coretta 2016) собраны данные длительности исландских гласных. Отфильтруйте данные, произнесенные носителем tt01 (переменная speaker), посчитайте байесовский коэффициент (\\(B_{12}\\)) для двух априорных моделей: нормального распределения со средним 87 и стандартным отклонением 25. (\\(m_1\\)) нормального распределения со средним 85 и стандартным отклонением 30. (\\(m_2\\)) Ответ округлите до трёх или менее знаков после запятой. "]]
